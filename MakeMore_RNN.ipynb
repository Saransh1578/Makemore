{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-L3VWQmHYPLQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=open('names.txt','r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njSXKFmFYxFx",
        "outputId": "f053421c-3d8c-4926-99a0-446bb4a06053"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxDfOrmKco35",
        "outputId": "e42e12aa-9cb2-4828-983c-52e91ed72f9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars=sorted(list(set(''.join(words))))\n",
        "stoi={s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.']=0\n",
        "itos={i:s for s,i in stoi.items()}\n",
        "vocab_size=len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6H63U5ecuGj",
        "outputId": "fafd1b51-6ab9-4382-a0bf-dcda9a59119b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the dataset\n",
        "block_size=3\n",
        "\n",
        "def build_dataset(words):\n",
        "  X,Y=[],[]\n",
        "  for w in words:\n",
        "    context= [0]*block_size\n",
        "    for ch in w+'.':\n",
        "      ix=stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context=context[1:]+[ix]\n",
        "\n",
        "  X=torch.tensor(X)\n",
        "  Y=torch.tensor(Y)\n",
        "  print(X.shape,Y.shape)\n",
        "  return X,Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1=int(0.8*len(words))\n",
        "n2=int(0.9*len(words))\n",
        "\n",
        "Xtr,Ytr=build_dataset(words[:n1])   #80%\n",
        "Xdev,Ydev=build_dataset(words[n1:n2])   #10%\n",
        "Xte,Yte=build_dataset(words[n2:])   #10%\n"
      ],
      "metadata": {
        "id": "MG65ukN9jwVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b68ad4-7fdf-4771-a324-b97648b40223"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP revisited\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd*block_size)**0.5)   #Kaiming init\n",
        "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
        "\n",
        "bngain=torch.ones((1, n_hidden))\n",
        "bnbias= torch.zeros((1 , n_hidden))\n",
        "bnmean_running= torch.zeros((1, n_hidden))\n",
        "bnstd_running= torch.ones((1, n_hidden))\n",
        "\n",
        "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkiVD4ESG3uj",
        "outputId": "71b54e70-72e5-4cc4-e0a0-4fb1927237e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
        "  #Applying batch normalization\n",
        "  bnmeani= hpreact.mean(0, keepdim=True)\n",
        "  bnstdi= hpreact.std(0, keepdim=True)\n",
        "  hpreact=bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
        "\n",
        "  with torch.no_grad():\n",
        "    bnmean_running=0.999 * bnmean_running + 0.001 * bnmeani\n",
        "    bnstd_running=0.999 * bnstd_running + 0.001 * bnstdi\n",
        "\n",
        "#Applying Non-linearity\n",
        "  h=torch.tanh(hpreact) #hidden layer\n",
        "  logits=h @ W2 + b2  #output layer\n",
        "  loss=F.cross_entropy(logits,Yb) #loss function\n",
        "\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  break;"
      ],
      "metadata": {
        "id": "E6xR6ITOHymm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7212003-6b5a-4be7-944a-46f71c45b5a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.3239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "ftd2M-5G5plH",
        "outputId": "ea7eb615-26ee-4124-f6d8-22fa1da61c1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7dca6d2b5510>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calibrate the batch norm at the end of the training\n",
        "with torch.no_grad():\n",
        "  #pass the training set\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 #+ b1\n",
        "  #measure the mean/std over the entire training set\n",
        "  bnmean= hpreact.mean(0, keepdim=True)\n",
        "  bnstd= hpreact.std(0, keepdim=True)"
      ],
      "metadata": {
        "id": "eiybHsZuGsex"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
        "  #hpreact=bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0 , keepdim=True) + bnbias\n",
        "  hpreact=bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
        "  h=torch.tanh(hpreact)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-BV8JTF8c9e",
        "outputId": "92119352-e372-4082-d64d-dec42f5f4a69"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 3.270019769668579\n",
            "val 3.269137144088745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g= torch.Generator().manual_seed(2147483647+10)\n",
        "\n",
        "for _ in range(20):\n",
        "  out=[]\n",
        "  context= [0] * block_size\n",
        "  while True:\n",
        "    emb=C[torch.tensor([context])]\n",
        "    h=torch.tanh(emb.view(1,-1) @ W1 + b1)\n",
        "    logits=h @ W2 + b2\n",
        "    probs=F.softmax(logits,dim=1)\n",
        "    #sample from the distribution\n",
        "\n",
        "    ix=torch.multinomial(probs,num_samples=1,generator=g).item()\n",
        "    context=context[1:]+[ix]\n",
        "    out.append(ix)\n",
        "    if ix==0:\n",
        "      break\n",
        "  print(''.join(itos[i] for i in out))\n"
      ],
      "metadata": {
        "id": "0SMuzYnB9rHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a Deeper network"
      ],
      "metadata": {
        "id": "US3G3HtjgVOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1, device=None):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    self.device = device if device is not None else torch.device(\"cpu\")\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim, device=self.device)\n",
        "    self.beta = torch.zeros(dim,  device=self.device)\n",
        "    # buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim , device=self.device)\n",
        "    self.running_var = torch.ones(dim, device=self.device)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True) # batch mean\n",
        "      xvar = x.var(0, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "layers = [\n",
        "  Linear(n_embd * block_size, n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, vocab_size),\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "  #last layer needs to be less confident\n",
        "  layers[-1].weight *=0.1\n",
        "  # all other layers: apply gain\n",
        "  for layer in layers[:-1]:\n",
        "    if isinstance(layer, Linear):\n",
        "      layer.weight *= 5/3\n",
        "\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n"
      ],
      "metadata": {
        "id": "R5rs2aSrCTts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d90e6e6-62eb-4388-d319-4256df381fe8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "ud = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for layer in layers:\n",
        "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  with torch.no_grad():\n",
        "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
        "\n",
        "  if i >= 1000:\n",
        "    break # AFTER_DEBUG: would take out obviously to run full optimization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vM6zvq0mEH7",
        "outputId": "5140c8b5-115b-4d8a-b8f5-1eb25f991918"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.2962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VT4_AXBMrz2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OQxi-Xh2qCGR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print no. of words, length of longest word, first 8 words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JShPSrZqjAb",
        "outputId": "b835a0e3-85e8-4c78-ae99-898384b3b9c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKOgrmOBqonx",
        "outputId": "6950b463-ce45-4cb3-ccb5-dba05bfc52b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVLOlVzTq30_",
        "outputId": "d15e5112-0371-42c2-d916-327a35c2a028"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comparing manual and pytorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "XEmb0E7Oq6V9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd=10\n",
        "n_hidden=64\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C  = torch.randn((vocab_size, n_embd),generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE9jFZYtlxts",
        "outputId": "c1436d6e-f328-4bde-b51e-2c444a1c27e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size\n",
        "#mini batch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ],
      "metadata": {
        "id": "0e3FLG3jsyt2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "id": "nIQ_oo504OoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d89fa98-5e5f-4af2-f700-aad40f0f144c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3475, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "dprobs = (1.0 / probs) * dlogprobs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = counts * dcounts\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw= bngain * dhpreact\n",
        "dbnbais= dhpreact.sum(0, keepdim=True)\n",
        "dbndiff =bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "dbndiff += (2*bndiff)*dbndiff2\n",
        "dhprebn= dbndiff.clone()\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb= dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j]\n",
        "\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h',dh,h)\n",
        "cmp('W2',dW2,W2)\n",
        "cmp('b2',db2,b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnbias', dbnbais, bnbias)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('C',dC, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-QXSOj96ki8",
        "outputId": "8e7aa688-ce73-4fa9-8d59-6fa52c24f7ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "metadata": {
        "id": "Trynx8TJEKH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f5ff9b-6f9a-49c0-8441-48f82934045a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.347533702850342 diff: -2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_logits= F.softmax(logits, 1)\n",
        "d_logits[range(n),Yb]-=1\n",
        "d_logits/=n\n",
        "\n",
        "cmp('logits', d_logits, logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDT8yL1uw_yX",
        "outputId": "13d3cf22-9bf9-481a-89d3-88e0ef0c282e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "VJxacP1mxH6r",
        "outputId": "f524b05c-35c3-4a1d-cc55-530450a8750d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x796cd9034d10>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5bz9INuzSLQ",
        "outputId": "1f17df7f-ecc4-4d70-9e1d-39eabebe6c16"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmAHKYhn-0NB",
        "outputId": "a21e4c29-06b2-4fe2-ce86-7696a0ceebdc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    # 2nd layer backprop\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # tanh\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    # batchnorm backprop\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "    # 1st layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    # embedding\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k,j]\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKjjteB--36E",
        "outputId": "aaea797e-6b43-4b32-e6f2-48d118379138"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.8159\n",
            "  10000/ 200000: 2.1930\n",
            "  20000/ 200000: 2.3867\n",
            "  30000/ 200000: 2.4378\n",
            "  40000/ 200000: 1.9755\n",
            "  50000/ 200000: 2.3740\n",
            "  60000/ 200000: 2.3899\n",
            "  70000/ 200000: 2.0956\n",
            "  80000/ 200000: 2.2740\n",
            "  90000/ 200000: 2.1282\n",
            " 100000/ 200000: 1.9677\n",
            " 110000/ 200000: 2.3097\n",
            " 120000/ 200000: 2.0231\n",
            " 130000/ 200000: 2.4189\n",
            " 140000/ 200000: 2.3073\n",
            " 150000/ 200000: 2.1730\n",
            " 160000/ 200000: 1.9353\n",
            " 170000/ 200000: 1.8548\n",
            " 180000/ 200000: 1.9951\n",
            " 190000/ 200000: 1.8593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ],
      "metadata": {
        "id": "GEIBofTbC6zH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiYSznkjDArd",
        "outputId": "2534d4cd-13c5-486a-a20d-39776485fe43"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.070988416671753\n",
            "val 2.1108741760253906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # ------------\n",
        "      # forward pass:\n",
        "      # Embedding\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # ------------\n",
        "      # Sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xnTUKZQDEB7",
        "outputId": "3927852e-46f0-4d35-b0d0-ebc882b79925"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mora.\n",
            "mayah.\n",
            "see.\n",
            "mad.\n",
            "ryla.\n",
            "reisha.\n",
            "ejdrie.\n",
            "cadelynnelin.\n",
            "shi.\n",
            "jen.\n",
            "eden.\n",
            "sana.\n",
            "arleigh.\n",
            "malaia.\n",
            "noshub.\n",
            "roshiriel.\n",
            "kindreelynn.\n",
            "novana.\n",
            "ubdence.\n",
            "ryyah.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jr--CpFlDIRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}